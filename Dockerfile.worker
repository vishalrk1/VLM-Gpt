FROM ubuntu:22.04

# Install only essential dependencies for llama-server and worker
RUN apt-get update && apt-get install -y --no-install-recommends \
    libopenblas0 \
    libgomp1 \
    libnuma1 \
    python3 \
    python3-pip \
    libclblast-dev \
    ninja-build \
    build-essential \
    git \
    wget \
    cmake \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Install minimal Python dependencies
RUN pip3 install --no-cache-dir redis fastapi uvicorn

RUN useradd -ms /bin/bash appuser

# Create work directory
WORKDIR /home/appuser/app

# Create models directory for downloading model files
RUN mkdir -p /home/appuser/app/models

# Comment out full project copy to reduce image size
# COPY . /home/appuser/app
# RUN wget https://huggingface.co/ggml-org/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-Q8_0.gguf?download=true -O /home/appuser/app/models/Qwen2.5-VL-3B-Instruct-Q8_0.gguf
# RUN wget https://huggingface.co/ggml-org/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/mmproj-Qwen2.5-VL-3B-Instruct-Q8_0.gguf?download=true -O /home/appuser/app/models/mmproj-Qwen2.5-VL-3B-Instruct-Q8_0.gguf
RUN git clone https://github.com/ggerganov/llama.cpp  

WORKDIR /home/appuser/app/llama.cpp
RUN cmake -B build -G Ninja -DLLAMA_CURL=OFF
RUN cmake --build build --config Release --target llama-server

# Fix permissions and add llama-server to PATH
RUN chown -R appuser:appuser /home/appuser/app/llama.cpp/build/bin
ENV PATH="/home/appuser/app/llama.cpp/build/bin:${PATH}"

# Switch back to app directory and copy only the worker file
WORKDIR /home/appuser/app
COPY workers/worker.py .

# Switch to non-root user for security
USER appuser

ENTRYPOINT ["python3", "worker.py"]
