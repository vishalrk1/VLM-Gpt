################################################# GPU DOCKERFILE #################################################

# FROM nvidia/cuda:12.1.1-devel-ubuntu22.04 AS builder
 
# RUN apt-get update && apt-get upgrade -y \
#     && apt-get install -y git build-essential \
#     python3 \
#     python3-pip \
#     gcc \
#     wget \
#     cmake \
#     ocl-icd-opencl-dev \
#     opencl-headers \
#     clinfo \
#     ninja-build \
#     libclblast-dev \
#     libopenblas-dev \
#     libgomp1 \
#     && mkdir -p /etc/OpenCL/vendors \
#     && echo "libnvidia-opencl.so.1" > /etc/OpenCL/vendors/nvidia.icd
 
# # setting build related env vars
# ENV CUDA_DOCKER_ARCH=all
# ENV LLAMA_CUBLAS=1
# ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/compat:$LD_LIBRARY_PATH
 
# WORKDIR /build
# RUN git clone --depth 1 https://github.com/ggml-org/llama.cpp.git  

# WORKDIR /build/llama.cpp
# RUN cmake -B build -G Ninja \
#     -DCMAKE_BUILD_TYPE=Release \
#     -DGGML_CUDA=ON \
#     -DCMAKE_CUDA_ARCHITECTURES="89" \
#     -DLLAMA_CURL=OFF \
#     -DLLAMA_BUILD_TESTS=OFF

# RUN cmake --build build --config Release --target llama-server
 
# FROM nvidia/cuda:12.1.1-runtime-ubuntu22.04
 
# # Set environment variable for CUDA library path
# RUN export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
 
# # Install apt packages
# RUN apt-get update && apt-get install -y --no-install-recommends \
#         libffi-dev \
#         libssl-dev \
#         poppler-utils \
#         libreoffice \
#         net-tools \
#         iproute2 \
#         ca-certificates \
#         libgomp1 \
#         wget && \
#     apt-get clean && rm -rf /var/lib/apt/lists/*

# # Install Miniconda
# RUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh     \
#     -O /tmp/miniconda.sh && \
#     /bin/bash /tmp/miniconda.sh -b -p /opt/miniconda && \
#     rm /tmp/miniconda.sh && \
#     /opt/miniconda/bin/conda init bash
# ENV PATH=/opt/miniconda/bin:$PATH

# # Create conda environment
# RUN conda tos accept --override-channels \
#         --channel defaults \
#         --channel pkgs/main \
#         --channel pkgs/r

# RUN conda update -n base conda && \
#     conda create -n llama python=3.9 -y

# # Copy built binaries from builder stage
# COPY --from=builder /build/llama.cpp /app/llama.cpp
# COPY --from=builder /build/llama.cpp/build/bin/* /usr/local/bin/
# COPY --from=builder /build/llama.cpp/build/bin/llama-server /usr/local/bin/
# COPY --from=builder /build/llama.cpp/build/bin/libllama.so /usr/local/lib/
# COPY --from=builder /build/llama.cpp/build/bin/libggml.so /usr/local/lib/
# COPY --from=builder /build/llama.cpp/build/bin/libggml-base.so /usr/local/lib/
# COPY --from=builder /build/llama.cpp/build/bin/libggml-cpu.so /usr/local/lib/
# COPY --from=builder /build/llama.cpp/build/bin/libggml-cuda.so /usr/local/lib/

# # Update library path
# RUN ldconfig
# ENV LD_LIBRARY_PATH=/app/llama.cpp/build/bin:$LD_LIBRARY_PATH

# SHELL ["/opt/miniconda/bin/conda", "run", "-n", "llama", "/bin/bash", "-c"]

# WORKDIR /app
# COPY . /app

# RUN pip install --no-cache-dir -r app_requirements.txt

# EXPOSE 5000 5002 7001

# # CMD ["conda", "run", "-n", "llama", "python", "vlm_app.py"]
# CMD ["conda", "run", "-n", "llama", "uvicorn", "api_v1:app", "--host", "0.0.0.0", "--port", "5000", "--log-level", "debug"]


# FROM python:3.11.5
# WORKDIR /app

# # Install build dependencies: build-essential includes GCC, G++, and make; libffi-dev for Foreign Function Interface (FFI); libssl-dev for SSL support
# RUN apt-get update && apt-get install -y --no-install-recommends \
#         libffi-dev \
#         libssl-dev \
#         poppler-utils \
#         libreoffice \
#         net-tools \
#         iproute2 \
#         ca-certificates \
#         libgomp1 \
#         wget && \
#     apt-get clean && rm -rf /var/lib/apt/lists/*

# # Copy built binaries from builder stage
# COPY --from=builder /build/llama.cpp /app/llama.cpp
# COPY --from=builder /build/llama.cpp/build/bin/* /usr/local/bin/
# COPY --from=builder /build/llama.cpp/build/bin/llama-server /usr/local/bin/
# COPY --from=builder /build/llama.cpp/build/bin/libllama.so /usr/local/lib/
# COPY --from=builder /build/llama.cpp/build/bin/libggml.so /usr/local/lib/
# COPY --from=builder /build/llama.cpp/build/bin/libggml-base.so /usr/local/lib/
# COPY --from=builder /build/llama.cpp/build/bin/libggml-cpu.so /usr/local/lib/
# COPY --from=builder /build/llama.cpp/build/bin/libggml-cuda.so /usr/local/lib/

# # Update library path
# RUN ldconfig
# ENV LD_LIBRARY_PATH=/app/llama.cpp/build/bin:$LD_LIBRARY_PATH

# WORKDIR /app
# COPY . /app

# RUN pip install -r app_requirements.txt
# EXPOSE 5000
# EXPOSE 5002
# EXPOSE 7001

# # Run application
# # CMD ["python", "vlm_app.py"]
# CMD ["uvicorn", "api_v1:app", "--host", "0.0.0.0", "--port", "5000"]


################################################# CPU DOCKERFILE #################################################

# FROM python:3.11.5
# WORKDIR /app

# # Install build dependencies: build-essential includes GCC, G++, and make; libffi-dev for Foreign Function Interface (FFI); libssl-dev for SSL support
# RUN apt-get update && apt-get install -y \
# 	build-essential \
# 	libffi-dev \
# 	libssl-dev \
# 	poppler-utils \
# 	libreoffice \
# 	git \
#     ninja-build \
# 	net-tools \
# 	iproute2 \
# 	cmake
# RUN libreoffice --version
# COPY . /app

# # Clone specific llama.cpp release
# RUN git clone https://github.com/ggerganov/llama.cpp  

# WORKDIR /app/llama.cpp
# RUN cmake -B build -G Ninja 
# RUN cmake --build build --config Release --target llama-server

# ENV PATH "/app/llama.cpp:${PATH}"
# WORKDIR /app

# RUN pip install -r app_requirements.txt
# EXPOSE 5000
# EXPOSE 5002
# EXPOSE 7001

# # Run application
# # CMD ["python", "vlm_app.py"]
# CMD ["uvicorn", "api_v1:app", "--host", "0.0.0.0", "--port", "5000"]



FROM ubuntu:22.04

# Install only RUNTIME dependencies, including the likely missing ones
RUN apt-get update && apt-get install -y --no-install-recommends \
    libopenblas0 \
    libgomp1 \
    libnuma1 \
    python3 \
    python3-pip \
    libclblast-dev \
    libopenblas-dev \
    ninja-build \
    build-essential \
	libffi-dev \
	libssl-dev \
	poppler-utils \
	libreoffice \
	git \
	net-tools \
	iproute2 \
	cmake \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

RUN pip3 install --no-cache-dir redis
# Add FastAPI and Uvicorn for the healthcheck endpoint
RUN pip3 install --no-cache-dir fastapi uvicorn

RUN useradd -ms /bin/bash appuser

# Create work directory
WORKDIR /home/appuser/app
COPY . /home/appuser/app

RUN git clone https://github.com/ggerganov/llama.cpp  

WORKDIR /home/appuser/app/llama.cpp
RUN cmake -B build -G Ninja -DLLAMA_CURL=OFF
RUN cmake --build build --config Release --target llama-server

# Fix permissions and add llama-server to PATH
RUN chown -R appuser:appuser /home/appuser/app/llama.cpp/build/bin
ENV PATH="/home/appuser/app/llama.cpp/build/bin:${PATH}"
WORKDIR /home/appuser/app

COPY workers/worker.py .
ENTRYPOINT ["python3", "worker.py"]
